{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "import os\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is filtering each file to get pure data , first we remove all the stop words from the file and then filter out the \\r the \\n and all the stop words from the files to get pure words that work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword={}\n",
    "s_words=open('stopwords.txt','r')\n",
    "s_words_data=s_words.read()\n",
    "for x in s_words_data.split('\\n'):\n",
    "    stopword[x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createArrayOfStrings(data):\n",
    "    \n",
    "    py=data.split(' ')\n",
    "    arr=[]\n",
    "    for x in range(len(py)):\n",
    "        word=''\n",
    "        word=py[x].replace('\\n','')\n",
    "        word=word.replace('\\r','')\n",
    "        word=word.replace('\\t','')\n",
    "        word=word.replace(',','')\n",
    "        word=word.replace('(','')\n",
    "        word=word.replace(').','')\n",
    "        word=word.replace(')','')\n",
    "        word=word.replace('.','')\n",
    "        word=word.replace('\"','')    \n",
    "        word=word.lower()\n",
    "        if word=='':\n",
    "            continue\n",
    "        elif word in stopword.keys():\n",
    "        \n",
    "            continue\n",
    "        else:\n",
    "            arr.append(word)\n",
    "    return arr\n",
    "\n",
    "def filterLabels(labels):\n",
    "    for x in range(len(labels)):\n",
    "        labels[x]=labels[x].replace('mini_newsgroups/','')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_newsgroups/alt.atheism\n",
      "mini_newsgroups/comp.graphics\n",
      "mini_newsgroups/comp.os.ms-windows.misc\n",
      "mini_newsgroups/comp.sys.ibm.pc.hardware\n",
      "mini_newsgroups/comp.sys.mac.hardware\n",
      "mini_newsgroups/comp.windows.x\n",
      "mini_newsgroups/misc.forsale\n",
      "mini_newsgroups/rec.autos\n",
      "mini_newsgroups/rec.motorcycles\n",
      "mini_newsgroups/rec.sport.baseball\n",
      "mini_newsgroups/rec.sport.hockey\n",
      "mini_newsgroups/sci.crypt\n",
      "mini_newsgroups/sci.electronics\n",
      "mini_newsgroups/sci.med\n",
      "mini_newsgroups/sci.space\n",
      "mini_newsgroups/soc.religion.christian\n",
      "mini_newsgroups/talk.politics.guns\n",
      "mini_newsgroups/talk.politics.mideast\n",
      "mini_newsgroups/talk.politics.misc\n",
      "mini_newsgroups/talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "'''path = 'mini_newsgroups/sci.space/'\n",
    "data=[]\n",
    "labels=[]\n",
    "for itemName in os.listdir(path):\n",
    "    itemName = os.path.join(path, itemName)\n",
    "    file_readed=open(itemName)\n",
    "    arr=createArrayOfStrings(file_readed.read())\n",
    "    data.append(arr)\n",
    "    labels.append(path)'''\n",
    "\n",
    "path = 'mini_newsgroups/'\n",
    "data=[]\n",
    "labels=[]\n",
    "for itemName in os.listdir(path):\n",
    "    itemName = os.path.join(path, itemName)\n",
    "    print itemName\n",
    "    for myPath in os.listdir(itemName):\n",
    "        myPath=os.path.join(itemName,myPath)\n",
    "        file_readed = open(myPath)\n",
    "        arr=createArrayOfStrings(file_readed.read())\n",
    "        data.append(arr)\n",
    "        labels.append(itemName)\n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24266\n"
     ]
    }
   ],
   "source": [
    "# so the data is now a 2d array, and now we are going to create a dictionary for every word in each data\n",
    "labels=filterLabels(labels)\n",
    "\n",
    "dictionary={}\n",
    "\n",
    "for x in data:\n",
    "    for y in x:\n",
    "        dictionary[y]=dictionary.get(y,0)+1 #if y not there then place 1 and y in the dictionary as Value and Key resp...\n",
    "\n",
    "# these are the features for the dataset and we are going to train the GNB on this dataset now!\n",
    "\n",
    "\n",
    "# remove the elements which have the value 1 in the dictionary\n",
    "for key in dictionary.keys():\n",
    "    if dictionary[key]==1:\n",
    "        del (dictionary[key])\n",
    "\n",
    "print len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a feature Matrix, where num features are number of words in the dictionary:\n",
    "feature_length=len(dictionary.keys())\n",
    "words_dictionary=dictionary.keys()\n",
    "#creating 2 D matrix now:\n",
    "X,noUse=np.mgrid[0:len(data), 0:len(dictionary.keys())] #this is a 2d matrix that we will now use\n",
    "for i in range(len(data)):\n",
    "    #this is string of one document, filtered of stop words, create a dictionary of words\n",
    "    d={}\n",
    "    for word in data[i]:\n",
    "        d[word]=d.get(word,0)+1\n",
    "        \n",
    "    for j in range(X.shape[1]): # in range X-> [ columns ]\n",
    "        X[i][j]=d.get(words_dictionary[j],0) # say, first word in 'Hello'. Now if 'Hello' exist 2 times in d{}. So this places 2 in X[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=ms.train_test_split(X,labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Scratch implementation of Naive bayes classifier\n",
    "\n",
    "def createDictionary(data,labels):\n",
    "     \n",
    "    dictionary={}\n",
    "    for x in data:\n",
    "        for y in x:\n",
    "            dictionary[y]=dictionary.get(y,0)+1 #if y not there then place 1 and y in the dictionary as Value and Key resp...\n",
    "\n",
    "    # these are the features for the dataset and we are going to train the GNB on this dataset now!\n",
    "    # remove the elements which have the value 1 in the dictionary\n",
    "    for key in dictionary.keys():\n",
    "        if dictionary[key]==1:\n",
    "            del (dictionary[key])\n",
    "    print 'created dictionary successfully'\n",
    "    return dictionary\n",
    "\n",
    "def createX(dictionary,data):\n",
    "    feature_length=len(dictionary.keys())\n",
    "    words_dictionary=dictionary.keys()\n",
    "    #creating 2 D matrix now:\n",
    "    X,noUse=np.mgrid[0:len(data), 0:len(dictionary.keys())] #this is a 2d matrix that we will now use\n",
    "    for i in range(len(data)):\n",
    "        #this is string of one document, filtered of stop words, create a dictionary of words\n",
    "        d={}\n",
    "        for word in data[i]:\n",
    "            d[word]=d.get(word,0)+1\n",
    "\n",
    "        for j in range(X.shape[1]): # in range X-> [ columns ]\n",
    "            X[i][j]=d.get(words_dictionary[j],0) # say, first word in 'Hello'. Now if 'Hello' exist 2 times in d{}. So this places 2 in X[0][0] \n",
    "    print 'created X successfully'\n",
    "    return X\n",
    "\n",
    "#given data, a 2d array of words and the corresponding classes of the data\n",
    "def fit(data,labels):\n",
    "    \n",
    "    dictionary=createDictionary(data,labels)\n",
    "    labels=filterLabels(labels)         \n",
    "    # Create a feature Matrix, where num features are number of words in the dictionary:\n",
    "    X=createX(dictionary,data)\n",
    "    print 'fit ran perfectly'\n",
    "    return X,dictionary\n",
    "\n",
    "def index(word,dictionary):\n",
    "    keys=dictionary.keys()\n",
    "    for i in range(len(dictionary.keys())):\n",
    "        if keys[i]==word:\n",
    "            return i\n",
    "    return -1\n",
    "    \n",
    "def predictOne(x,X,labels,dictionary):\n",
    "    max_prob=0\n",
    "    class_prob=''\n",
    "    for y in set(labels):\n",
    "        prob=0\n",
    "        #-------------------------------------\n",
    "        arr=np.zeros(len(labels))\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i]==y:\n",
    "                arr[i]=1\n",
    "        x_=np.array(arr, dtype=bool)\n",
    "        # -----------------------------------\n",
    "        X_small=X[x_] # this is wrong but replace this with the right function, that should be very easy\n",
    "        \n",
    "        #countAllWordsOfClass=sum(map(sum, X_small))\n",
    "        print '1'\n",
    "        for word in (x):\n",
    "            if index(word,dictionary)!=-1:\n",
    "                prob+=np.log(X_small[:,index(word,dictionary)].sum() +1)#- np.log(countAllWordsOfClass)\n",
    "        prob=prob+np.log(1.0/20.0) #because prob of each class is 1/20\n",
    "        if prob>max_prob:\n",
    "            max_prob=prob\n",
    "            class_prob=y\n",
    "    return class_prob\n",
    "\n",
    "def predict(X_test,X,labels,dictionary):\n",
    "    ypred=[]\n",
    "    for x in X_test:\n",
    "        pred=predictOne(x,X,labels,dictionary)\n",
    "        print pred\n",
    "        ypred.append(pred)\n",
    "    return ypred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "talk.religion.misc\n",
      "['talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#X,dictionary=fit(data,labels)\n",
    "print predict(data[1951:1952],X,labels,dictionary)\n",
    "#run()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.90      0.83      0.86        23\n",
      "           comp.graphics       0.65      0.71      0.68        24\n",
      " comp.os.ms-windows.misc       0.53      0.73      0.62        22\n",
      "comp.sys.ibm.pc.hardware       0.96      0.90      0.93        29\n",
      "   comp.sys.mac.hardware       0.84      0.81      0.82        26\n",
      "          comp.windows.x       0.91      0.72      0.81        29\n",
      "            misc.forsale       0.60      0.72      0.65        25\n",
      "               rec.autos       0.77      0.89      0.83        27\n",
      "         rec.motorcycles       0.90      1.00      0.95        18\n",
      "      rec.sport.baseball       1.00      0.90      0.95        31\n",
      "        rec.sport.hockey       1.00      1.00      1.00        23\n",
      "               sci.crypt       0.80      0.64      0.71        25\n",
      "         sci.electronics       0.96      0.69      0.80        32\n",
      "                 sci.med       0.76      0.73      0.75        26\n",
      "               sci.space       1.00      0.72      0.84        25\n",
      "  soc.religion.christian       0.93      1.00      0.96        25\n",
      "      talk.politics.guns       0.82      0.78      0.80        23\n",
      "   talk.politics.mideast       0.71      0.71      0.71        21\n",
      "      talk.politics.misc       0.30      0.38      0.33        24\n",
      "      talk.religion.misc       0.43      0.59      0.50        22\n",
      "\n",
      "             avg / total       0.80      0.77      0.78       500\n",
      "\n",
      "[[19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  3]\n",
      " [ 0 17  2  0  0  0  0  1  1  0  0  2  1  0  0  0  0  0  0  0]\n",
      " [ 0  2 16  1  0  1  1  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  1 26  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  2  0 21  0  1  0  1  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  2  0  1 21  1  0  0  0  0  0  0  0  0  1  0  1  0  2]\n",
      " [ 0  1  0  0  1  0 18  1  0  0  0  0  0  1  0  0  0  0  3  0]\n",
      " [ 1  0  0  0  0  0  2 24  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  1  0  0  0  0 28  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 23  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  0  0  0  0  1  0  0  0 16  0  1  0  1  1  0  2  1]\n",
      " [ 0  2  0  0  0  0  3  1  0  0  0  0 22  0  0  0  0  1  1  2]\n",
      " [ 0  1  0  0  0  1  0  1  0  0  0  0  0 19  0  0  0  0  2  2]\n",
      " [ 0  0  2  0  0  0  2  0  0  0  0  0  0  1 18  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0 18  1  2  1]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  3  2]\n",
      " [ 0  1  2  0  1  0  2  1  0  0  0  0  0  2  0  0  1  2  9  3]\n",
      " [ 1  1  0  0  0  0  0  0  0  0  0  1  0  1  0  0  1  1  3 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print classification_report(Y_test,ypred)\n",
    "print confusion_matrix(Y_test,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float, int, None, optional\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. By default, the value is set to 0.25.\n",
      "        The default will change in version 0.21. It will remain 0.25 only\n",
      "        if ``train_size`` is unspecified, otherwise it will complement\n",
      "        the specified ``train_size``.\n",
      "    \n",
      "    train_size : float, int, or None, default None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "    \n",
      "    shuffle : boolean, optional (default=True)\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like or None (default is None)\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ms.train_test_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
