{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "import os\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is filtering each file to get pure data , first we remove all the stop words from the file and then filter out the \\r the \\n and all the stop words from the files to get pure words that work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword={}\n",
    "s_words=open('stopwords.txt','r')\n",
    "s_words_data=s_words.read()\n",
    "for x in s_words_data.split('\\n'):\n",
    "    stopword[x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createArrayOfStrings(data):\n",
    "    \n",
    "    py=data.split(' ')\n",
    "    arr=[]\n",
    "    for x in range(len(py)):\n",
    "        word=''\n",
    "        word=py[x].replace('\\n','')\n",
    "        word=word.replace('\\r','')\n",
    "        word=word.replace('\\t','')\n",
    "        word=word.replace(',','')\n",
    "        word=word.replace('(','')\n",
    "        word=word.replace(').','')\n",
    "        word=word.replace(')','')\n",
    "        word=word.replace('.','')\n",
    "        word=word.replace('\"','')    \n",
    "        word=word.lower()\n",
    "        if word=='':\n",
    "            continue\n",
    "        elif word in stopword.keys():\n",
    "        \n",
    "            continue\n",
    "        else:\n",
    "            arr.append(word)\n",
    "    return arr\n",
    "\n",
    "def filterLabels(labels):\n",
    "    for x in range(len(labels)):\n",
    "        labels[x]=labels[x].replace('mini_newsgroups/','')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_newsgroups/alt.atheism\n",
      "mini_newsgroups/comp.graphics\n",
      "mini_newsgroups/comp.os.ms-windows.misc\n",
      "mini_newsgroups/comp.sys.ibm.pc.hardware\n",
      "mini_newsgroups/comp.sys.mac.hardware\n",
      "mini_newsgroups/comp.windows.x\n",
      "mini_newsgroups/misc.forsale\n",
      "mini_newsgroups/rec.autos\n",
      "mini_newsgroups/rec.motorcycles\n",
      "mini_newsgroups/rec.sport.baseball\n",
      "mini_newsgroups/rec.sport.hockey\n",
      "mini_newsgroups/sci.crypt\n",
      "mini_newsgroups/sci.electronics\n",
      "mini_newsgroups/sci.med\n",
      "mini_newsgroups/sci.space\n",
      "mini_newsgroups/soc.religion.christian\n",
      "mini_newsgroups/talk.politics.guns\n",
      "mini_newsgroups/talk.politics.mideast\n",
      "mini_newsgroups/talk.politics.misc\n",
      "mini_newsgroups/talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "'''path = 'mini_newsgroups/sci.space/'\n",
    "data=[]\n",
    "labels=[]\n",
    "for itemName in os.listdir(path):\n",
    "    itemName = os.path.join(path, itemName)\n",
    "    file_readed=open(itemName)\n",
    "    arr=createArrayOfStrings(file_readed.read())\n",
    "    data.append(arr)\n",
    "    labels.append(path)'''\n",
    "\n",
    "path = 'mini_newsgroups/'\n",
    "data=[]\n",
    "labels=[]\n",
    "for itemName in os.listdir(path):\n",
    "    itemName = os.path.join(path, itemName)\n",
    "    print itemName\n",
    "    for myPath in os.listdir(itemName):\n",
    "        myPath=os.path.join(itemName,myPath)\n",
    "        file_readed = open(myPath)\n",
    "        arr=createArrayOfStrings(file_readed.read())\n",
    "        data.append(arr)\n",
    "        labels.append(itemName)\n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24266\n"
     ]
    }
   ],
   "source": [
    "# so the data is now a 2d array, and now we are going to create a dictionary for every word in each data\n",
    "labels=filterLabels(labels)\n",
    "\n",
    "dictionary={}\n",
    "\n",
    "for x in data:\n",
    "    for y in x:\n",
    "        dictionary[y]=dictionary.get(y,0)+1 #if y not there then place 1 and y in the dictionary as Value and Key resp...\n",
    "\n",
    "# these are the features for the dataset and we are going to train the GNB on this dataset now!\n",
    "\n",
    "\n",
    "# remove the elements which have the value 1 in the dictionary\n",
    "for key in dictionary.keys():\n",
    "    if dictionary[key]==1:\n",
    "        del (dictionary[key])\n",
    "\n",
    "print len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a feature Matrix, where num features are number of words in the dictionary:\n",
    "feature_length=len(dictionary.keys())\n",
    "words_dictionary=dictionary.keys()\n",
    "#creating 2 D matrix now:\n",
    "X,noUse=np.mgrid[0:len(data), 0:len(dictionary.keys())] #this is a 2d matrix that we will now use\n",
    "for i in range(len(data)):\n",
    "    #this is string of one document, filtered of stop words, create a dictionary of words\n",
    "    d={}\n",
    "    for word in data[i]:\n",
    "        d[word]=d.get(word,0)+1\n",
    "        \n",
    "    for j in range(X.shape[1]): # in range X-> [ columns ]\n",
    "        X[i][j]=d.get(words_dictionary[j],0) # say, first word in 'Hello'. Now if 'Hello' exist 2 times in d{}. So this places 2 in X[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=ms.train_test_split(X,labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Scratch implementation of Naive bayes classifier\n",
    "\n",
    "def createDictionary(data,labels):\n",
    "     \n",
    "    dictionary={}\n",
    "    for x in data:\n",
    "        for y in x:\n",
    "            dictionary[y]=dictionary.get(y,0)+1 #if y not there then place 1 and y in the dictionary as Value and Key resp...\n",
    "\n",
    "    # these are the features for the dataset and we are going to train the GNB on this dataset now!\n",
    "    # remove the elements which have the value 1 in the dictionary\n",
    "    for key in dictionary.keys():\n",
    "        if dictionary[key]==1:\n",
    "            del (dictionary[key])\n",
    "    print 'created dictionary successfully'\n",
    "    return dictionary\n",
    "\n",
    "def createX(dictionary,data):\n",
    "    feature_length=len(dictionary.keys())\n",
    "    words_dictionary=dictionary.keys()\n",
    "    #creating 2 D matrix now:\n",
    "    X,noUse=np.mgrid[0:len(data), 0:len(dictionary.keys())] #this is a 2d matrix that we will now use\n",
    "    for i in range(len(data)):\n",
    "        #this is string of one document, filtered of stop words, create a dictionary of words\n",
    "        d={}\n",
    "        for word in data[i]:\n",
    "            d[word]=d.get(word,0)+1\n",
    "\n",
    "        for j in range(X.shape[1]): # in range X-> [ columns ]\n",
    "            X[i][j]=d.get(words_dictionary[j],0) # say, first word in 'Hello'. Now if 'Hello' exist 2 times in d{}. So this places 2 in X[0][0] \n",
    "    print 'created X successfully'\n",
    "    return X\n",
    "\n",
    "#given data, a 2d array of words and the corresponding classes of the data\n",
    "def fit(data,labels):\n",
    "    \n",
    "    dictionary=createDictionary(data,labels)\n",
    "    labels=filterLabels(labels)         \n",
    "    # Create a feature Matrix, where num features are number of words in the dictionary:\n",
    "    X=createX(dictionary,data)\n",
    "    print 'fit ran perfectly'\n",
    "    return X,dictionary\n",
    "\n",
    "def index(word,dictionary):\n",
    "    keys=dictionary.keys()\n",
    "    for i in range(len(dictionary.keys())):\n",
    "        if keys[i]==word:\n",
    "            return i\n",
    "    return -1\n",
    "    \n",
    "def createX_Counts(X,labels,dictSize):\n",
    "    X_counts=np.zeros((len(set(labels)),dictSize))\n",
    "    i=0\n",
    "    for y in set(labels):\n",
    "        prob=0\n",
    "        #-------------------------------------\n",
    "        arr=np.zeros(len(labels))\n",
    "        for i_ in range(len(labels)):\n",
    "            if labels[i_]==y:\n",
    "                arr[i_]=1\n",
    "        x_=np.array(arr, dtype=bool)\n",
    "        # -----------------------------------\n",
    "        X_small=X[x_]\n",
    "        for j in range(dictSize):\n",
    "            X_counts[i][j]=X_small[:,j].sum()\n",
    "        \n",
    "        i+=1\n",
    "    print len(X_counts)\n",
    "    return X_counts\n",
    "\n",
    "\n",
    "def predictOne(x,X_counts,labels,dictionary):\n",
    "    max_prob=0\n",
    "    class_prob=''\n",
    "    current_class=0\n",
    "    \n",
    "    for i in range(len(set(labels))):\n",
    "        prob=0\n",
    "        \n",
    "        #countAllWordsOfClass=sum(map(sum, X_small))\n",
    "        print '1'\n",
    "        for word in (x):\n",
    "            if index(word,dictionary)!=-1:\n",
    "                prob+=np.log(X_counts[i,index(word,dictionary)] + 1)#- np.log(countAllWordsOfClass)\n",
    "        prob=prob+np.log(1.0/20.0) #because prob of each class is 1/20\n",
    "        if prob>max_prob:\n",
    "            max_prob=prob\n",
    "            class_prob=list(set(labels))[i]\n",
    "    return class_prob\n",
    "\n",
    "def predict(X_test,X,labels,dictionary):\n",
    "    ypred=[]\n",
    "    for x in X_test:\n",
    "        pred=predictOne(x,X,labels,dictionary)\n",
    "        print pred\n",
    "        ypred.append(pred)\n",
    "    return ypred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\80LM0141IH\\Anaconda3\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "soc.religion.christian\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "soc.religion.christian\n",
      "['soc.religion.christian', 'soc.religion.christian']\n"
     ]
    }
   ],
   "source": [
    "#X,dictionary=fit(data,labels)\n",
    "X_counts= createX_Counts(X,labels,len(dictionary))\n",
    "\n",
    "print predict(data[1531:1533],X_counts,labels,dictionary)\n",
    "#run()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ypred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e3f0ce7043b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ypred' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.sys.mac.hardware'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=np.mgrid[0:5,0:5]\n",
    "b.sum()\n",
    "list(set(labels))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
